{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb66dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import torchvision for datasets and transforms\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import DataLoader for batching\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dade1628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for the dataset (convert to tensor and normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2860,), (0.3530,))\n",
    "])\n",
    "\n",
    "# Download and load the training dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Download and load the test dataset\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21d242b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 60000\n",
      "Number of test samples: 10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the number of samples in the training and test datasets\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Visualize a sample image and its label\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "plt.imshow(images[0].squeeze(), cmap='gray')\n",
    "plt.title(f\"Label: {labels[0].item()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816097e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Feedforward Neural Network with more neurons and 4 extra hidden layer\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.relu1 = nn.LeakyReLU(0.01)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.relu2 = nn.LeakyReLU(0.01)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.relu3 = nn.LeakyReLU(0.01)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.relu4 = nn.LeakyReLU(0.01)\n",
    "        self.fc5 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # Weight initialization\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.fc4.weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc0f2453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardNN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
      "  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): LeakyReLU(negative_slope=0.01)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): LeakyReLU(negative_slope=0.01)\n",
      "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): LeakyReLU(negative_slope=0.01)\n",
      "  (fc4): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (bn4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu4): LeakyReLU(negative_slope=0.01)\n",
      "  (fc5): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = FeedforwardNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc8802bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedforwardNN(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
       "  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): LeakyReLU(negative_slope=0.01)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): LeakyReLU(negative_slope=0.01)\n",
       "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): LeakyReLU(negative_slope=0.01)\n",
       "  (fc4): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (bn4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu4): LeakyReLU(negative_slope=0.01)\n",
       "  (fc5): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffac019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Print the device the model is on\n",
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1770a2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/60] Train Loss: 0.8107, Train Acc: 77.01% | Test Loss: 0.4417, Test Acc: 84.68%\n",
      "Epoch [2/60] Train Loss: 0.4492, Train Acc: 85.00% | Test Loss: 0.3771, Test Acc: 86.41%\n",
      "Epoch [3/60] Train Loss: 0.3845, Train Acc: 86.75% | Test Loss: 0.3476, Test Acc: 87.36%\n",
      "Epoch [4/60] Train Loss: 0.3531, Train Acc: 87.48% | Test Loss: 0.3343, Test Acc: 87.72%\n",
      "Epoch [5/60] Train Loss: 0.3301, Train Acc: 88.30% | Test Loss: 0.3233, Test Acc: 88.44%\n",
      "Epoch [6/60] Train Loss: 0.3118, Train Acc: 88.78% | Test Loss: 0.3152, Test Acc: 88.59%\n",
      "Epoch [7/60] Train Loss: 0.2957, Train Acc: 89.39% | Test Loss: 0.3089, Test Acc: 88.84%\n",
      "Epoch [8/60] Train Loss: 0.2797, Train Acc: 89.89% | Test Loss: 0.3016, Test Acc: 89.09%\n",
      "Epoch [9/60] Train Loss: 0.2700, Train Acc: 90.34% | Test Loss: 0.3022, Test Acc: 89.01%\n",
      "Epoch [10/60] Train Loss: 0.2601, Train Acc: 90.53% | Test Loss: 0.2982, Test Acc: 89.38%\n",
      "Epoch [11/60] Train Loss: 0.2486, Train Acc: 90.95% | Test Loss: 0.2923, Test Acc: 89.54%\n",
      "Epoch [12/60] Train Loss: 0.2385, Train Acc: 91.38% | Test Loss: 0.2922, Test Acc: 89.70%\n",
      "Epoch [13/60] Train Loss: 0.2289, Train Acc: 91.64% | Test Loss: 0.2994, Test Acc: 89.54%\n",
      "Epoch [14/60] Train Loss: 0.2250, Train Acc: 91.81% | Test Loss: 0.2872, Test Acc: 90.13%\n",
      "Epoch [15/60] Train Loss: 0.2161, Train Acc: 92.13% | Test Loss: 0.2891, Test Acc: 90.08%\n",
      "Epoch [16/60] Train Loss: 0.2068, Train Acc: 92.51% | Test Loss: 0.2901, Test Acc: 90.04%\n",
      "Epoch [17/60] Train Loss: 0.2008, Train Acc: 92.73% | Test Loss: 0.3034, Test Acc: 89.81%\n",
      "Epoch [18/60] Train Loss: 0.1973, Train Acc: 92.78% | Test Loss: 0.2970, Test Acc: 89.79%\n",
      "Epoch [19/60] Train Loss: 0.1886, Train Acc: 93.06% | Test Loss: 0.2920, Test Acc: 90.07%\n",
      "Epoch [20/60] Train Loss: 0.1845, Train Acc: 93.20% | Test Loss: 0.2919, Test Acc: 89.84%\n",
      "Epoch [21/60] Train Loss: 0.1788, Train Acc: 93.46% | Test Loss: 0.2891, Test Acc: 90.12%\n",
      "Epoch [22/60] Train Loss: 0.1719, Train Acc: 93.79% | Test Loss: 0.3041, Test Acc: 89.85%\n",
      "Epoch [23/60] Train Loss: 0.1665, Train Acc: 93.91% | Test Loss: 0.2942, Test Acc: 90.22%\n",
      "Epoch [24/60] Train Loss: 0.1636, Train Acc: 94.03% | Test Loss: 0.3048, Test Acc: 90.10%\n",
      "Epoch [25/60] Train Loss: 0.1595, Train Acc: 94.13% | Test Loss: 0.3061, Test Acc: 90.04%\n",
      "Epoch [26/60] Train Loss: 0.1517, Train Acc: 94.53% | Test Loss: 0.3051, Test Acc: 90.21%\n",
      "Epoch [27/60] Train Loss: 0.1487, Train Acc: 94.58% | Test Loss: 0.3043, Test Acc: 90.14%\n",
      "Epoch [28/60] Train Loss: 0.1471, Train Acc: 94.65% | Test Loss: 0.3029, Test Acc: 90.24%\n",
      "Epoch [29/60] Train Loss: 0.1422, Train Acc: 94.88% | Test Loss: 0.3013, Test Acc: 90.38%\n",
      "Epoch [30/60] Train Loss: 0.1374, Train Acc: 94.95% | Test Loss: 0.3026, Test Acc: 90.57%\n",
      "Epoch [31/60] Train Loss: 0.1339, Train Acc: 95.08% | Test Loss: 0.3008, Test Acc: 90.71%\n",
      "Epoch [32/60] Train Loss: 0.1300, Train Acc: 95.25% | Test Loss: 0.3096, Test Acc: 90.34%\n",
      "Epoch [33/60] Train Loss: 0.1267, Train Acc: 95.41% | Test Loss: 0.3145, Test Acc: 90.36%\n",
      "Epoch [34/60] Train Loss: 0.1231, Train Acc: 95.42% | Test Loss: 0.3147, Test Acc: 90.39%\n",
      "Epoch [35/60] Train Loss: 0.1217, Train Acc: 95.49% | Test Loss: 0.3143, Test Acc: 90.43%\n",
      "Epoch [36/60] Train Loss: 0.1162, Train Acc: 95.71% | Test Loss: 0.3186, Test Acc: 90.49%\n",
      "Epoch [37/60] Train Loss: 0.1129, Train Acc: 95.92% | Test Loss: 0.3142, Test Acc: 90.77%\n",
      "Epoch [38/60] Train Loss: 0.1106, Train Acc: 95.90% | Test Loss: 0.3265, Test Acc: 90.30%\n",
      "Epoch [39/60] Train Loss: 0.1076, Train Acc: 96.07% | Test Loss: 0.3259, Test Acc: 90.51%\n",
      "Epoch [40/60] Train Loss: 0.1059, Train Acc: 96.12% | Test Loss: 0.3305, Test Acc: 90.22%\n",
      "Epoch [41/60] Train Loss: 0.1029, Train Acc: 96.21% | Test Loss: 0.3235, Test Acc: 90.62%\n",
      "Epoch [42/60] Train Loss: 0.1008, Train Acc: 96.35% | Test Loss: 0.3266, Test Acc: 90.55%\n",
      "Epoch [43/60] Train Loss: 0.0998, Train Acc: 96.29% | Test Loss: 0.3231, Test Acc: 90.40%\n",
      "Epoch [44/60] Train Loss: 0.0935, Train Acc: 96.66% | Test Loss: 0.3423, Test Acc: 90.33%\n",
      "Epoch [45/60] Train Loss: 0.0948, Train Acc: 96.61% | Test Loss: 0.3359, Test Acc: 90.51%\n",
      "Epoch [46/60] Train Loss: 0.0903, Train Acc: 96.71% | Test Loss: 0.3380, Test Acc: 90.60%\n",
      "Epoch [47/60] Train Loss: 0.0915, Train Acc: 96.63% | Test Loss: 0.3386, Test Acc: 90.34%\n",
      "Epoch [48/60] Train Loss: 0.0867, Train Acc: 96.81% | Test Loss: 0.3469, Test Acc: 90.67%\n",
      "Epoch [49/60] Train Loss: 0.0859, Train Acc: 96.82% | Test Loss: 0.3489, Test Acc: 90.34%\n",
      "Epoch [50/60] Train Loss: 0.0842, Train Acc: 96.86% | Test Loss: 0.3461, Test Acc: 90.70%\n",
      "Epoch [51/60] Train Loss: 0.0851, Train Acc: 96.86% | Test Loss: 0.3431, Test Acc: 90.41%\n",
      "Epoch [52/60] Train Loss: 0.0839, Train Acc: 96.91% | Test Loss: 0.3476, Test Acc: 90.89%\n",
      "Epoch [53/60] Train Loss: 0.0791, Train Acc: 97.11% | Test Loss: 0.3481, Test Acc: 90.50%\n",
      "Epoch [54/60] Train Loss: 0.0800, Train Acc: 97.00% | Test Loss: 0.3584, Test Acc: 90.52%\n",
      "Epoch [55/60] Train Loss: 0.0742, Train Acc: 97.28% | Test Loss: 0.3574, Test Acc: 90.54%\n",
      "Epoch [56/60] Train Loss: 0.0748, Train Acc: 97.25% | Test Loss: 0.3624, Test Acc: 90.46%\n",
      "Epoch [57/60] Train Loss: 0.0750, Train Acc: 97.26% | Test Loss: 0.3772, Test Acc: 90.16%\n",
      "Epoch [58/60] Train Loss: 0.0709, Train Acc: 97.42% | Test Loss: 0.3662, Test Acc: 90.42%\n",
      "Epoch [59/60] Train Loss: 0.0734, Train Acc: 97.36% | Test Loss: 0.3708, Test Acc: 90.48%\n",
      "Epoch [60/60] Train Loss: 0.0701, Train Acc: 97.41% | Test Loss: 0.3686, Test Acc: 90.35%\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "\n",
    "# Training loop with overfitting check\n",
    "num_epochs = 60\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct_train / total_train\n",
    "\n",
    "    # Evaluate on test set after each epoch\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_acc = 100 * correct_test / total_test\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc107ba0",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eaee07",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Reflection**\n",
    "\n",
    "For the first attempt, I implemented a neural network model without utilizing techniques such as weight initialization, input standardization, and batch normalization. The test accuracy I achieved at that time was **88.31%**. After applying the above techniques and performing hyperparameter tuning, the final test accuracy improved to **90.35%**, which is a decent value. However, it did not meet the desired target of at least **91%**. Below is a discussion of how these techniques contributed to improving the model:\n",
    "\n",
    "---\n",
    "\n",
    "### **Weight Initialization**\n",
    "With proper weight initialization, my model was able to learn more effectively from the start. Initially, I used Xavier (Glorot) initialization, which is not ideal for ReLU-based neurons. This resulted in only a slight improvement to 88.97%. Once I switched to Kaiming (He) initialization, combined with LeakyReLU activation, the model achieved more stable convergence and improved gradient flow, leading to better accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### **Batch Normalization**\n",
    "Batch normalization was added after each linear layer to stabilize and speed up training. It reduced internal covariate shift and allowed the model to generalize better, directly boosting test accuracy. This technique also acted as a regularizer, minimizing overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### **Input Standardization**\n",
    "I normalized the FashionMNIST images using the dataset's mean and standard deviation. This ensured that all input features were on a similar scale, helping the model train faster and generalize better. Input standardization played a significant role in improving both training and test accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### **Challenges**\n",
    "Initially, I configured the model with only two hidden layers (128 and 64 neurons) to keep it simple. However, the neuron capacity was insufficient for achieving decent results, and the accuracy plateaued at around 89%. To address this, I increased the model's capacity by adding more hidden layers (1024, 512, 256, 128 neurons) and used dropout to reduce overfitting. Additionally, I added L2 regularization (weight decay) to further reduce overfitting for this complex model structure.\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Thoughts**\n",
    "Although the desired test accuracy of **91%** was not achieved, the techniques applied significantly improved the model's performance from **88.31%** to **90.35%**. The journey involved systematic experimentation, hyperparameter tuning, and careful monitoring of training and test accuracy. This process has demonstrated the importance of applying best practices in deep learning to build robust models that generalize well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-tf-torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
